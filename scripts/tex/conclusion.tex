MetaModel means CMA equipped with a MetaModel, which is automatically enabled when the learning performs well.
ChainMetaModelSQP is a memetic algorithm: it combines MetaModel (hence, CMA plus a MetaModel) and a final run using a local method, namely SQP (sequential quadratic programming).
Usually, a meta-model and a fast local search a the end do improve evolutionary methods.

On benchmarks close to the good old BBOB, CMA and variants do perform well. The variants equipped with a MetaModel perform better, and
variants equipped with the the MetaModel and the final local search are even better.

On many real-world benchmarks, the budget is lower than in the traditional context of BBOB with budget $=$ dimension $\times$ 1000. There
are cases with a ratio budget/dimension $<1$, of the order of a few units or a few dozens. DE performs well in many cases. This is consistent with many publications considering real-world problems. Our real-world benchmarks include TODO

SQP is excellent in noisy optimization. 

We include benchmarks with one or several or many constraints (prefix onepen, pen and megapen), tackled with dynamic penalization: results were not fundamentally different from the non-penalized case. However, MetaModels are effective in a very stable and visible manner: this is consistent with the state of the art TODO

MetaModel perform well on BBOB-style optimization, but were also excellent for several low budget things.

Regarding discrete contexts, we note the great performance of the so-called ``DiscreteLenglerOnePlusOne'' \cite{lengler}. We tested variants with a different constant and it turns out that the proved constant, in spite of the simple context in which it was derived, is good.
Still in the discrete case, we note the good performance of methods with ``Recombining'' in the name: while it was less investigated theoretically than variants of the discrete $(1+1)$ method, methods with crossover might deserve more work.

Regarding the principles of benchmarking, we note that the two different views in Nevergrad (the heatmap and the average normalized loss) present completely different views. This emphasizes how much how we look at data has a big impact on the interpretation.

Regarding BBOB variants, TODO

Regarding PBBOB, TODO

Consistently with some real world experiments in \cite{micropredictions1,micropredictions2}, we note the the $(1+1)$ evolution strategy with one-fifth rule from \cite{rechenberg73} is still quite good. In artificial benchmarks with a lot of evaluations, high conditionning and artificially rotated contexts, it can become weak: for many realistic contexts, in particular a realistic ratio budget/dimension, it is quite good.

\subsection{Caveats, further work}
Some benchmarks were implemented but not included in the release due to legal issues in the license. 
We did not include the important case in which a multi-objective run is performed on surrogate models only: (1) randomly sample, (2) approximate the objective functions by surrogate models, (3) perform a multi-objective optimization on the surrogate only. This is useful for including the user in the loop. This is not tested in the current benchmarks.

Compared to the old Dashboard from 2021, results are somehow similar, with more details. However, we have more real world benchmarks and more discrete experiments. Also,
some methods have been removed, in particular some slow methods which were rarely performing well compared to present methods.
