\documentclass{article}


\usepackage{graphicx}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}

\def\E{\mathbb{E}}
\begin{document}

\title{Dagsthloid benchmarking}

\maketitle

\tableofcontents

\begin{abstract}
At the Dagstuhl seminar 23251 (June 2023), many principles of benchmarking were discussed.
We keep in mind the following guidelines:
We want everything to be easy to rerun entirely from scratch. This might need a significant computational power.
However, it is possible to run manually and separately some benchmarks. One can easily edit the main script and reduce the number of benchmarks, possibly to a single benchmark at a time.


\end{abstract}

\section{Introduction}

In artificial benchmarks, some talks pointed out the importance of the distribution of optima. Therefore, we take into account the parameters of the distribution of optima.
We compare implementations, not abstract algorithms. The detailed implementations are freely available in Nevergrad and anyone can propose a modification.
A large part of the benchmarks are based on real-world data or reasonably close to this.
Benchmarking is not a joke. Recent news show that poorly reproducible benchmarking is detrimental to science.

Naming: “oid” means “similar to”. The benchmark is called Dagstuhloid because it is inspired by Dagstuhl talks. The responsibility is entirely ours, though.

The main place for discussing the Dagstuhloid benchmark is \url{https://www.facebook.com/groups/nevergradusers/}. 

The code is https://github.com/facebookresearch/nevergrad

How to reproduce these results:
\begin{itemize}
\item Install Nevergrad by “git clone” (see details at the URL above).
\item  Running:
\begin{itemize}
\item Without cluster: << python -m nevergrad.benchmark yabbob --num\_workers=67 >> if you want to run YABBOB on 67 cores. Other benchmarks include TODO
\item With cluster: Run “sbatch dagstuhloid.sh” for launching experiments with Slurm. With other cluster management tools, you might have to adapt the script. It is written assuming slurm: it should be feasible to adapt it to other benchmarks. This script is randomized: you might have to run it several times for getting enough results.
\end{itemize}
\item For plotting results, run “dagstuhloid\_plot.sh”. Of course, some data might be missing if not enough runs are complete. Note that in case of latex unavailable or incompatible, all figures are nonetheless available.
\end{itemize}

\section{Comparison with other benchmarks}

\section{Experimental results}

For each benchmark, the detailed setup is documented at \url{https://github.com/facebookresearch/nevergrad/blob/main/nevergrad/benchmark/experiments.py}.
Ctrl-F with the name of the benchmark should provide all details.
For each benchmark we provide both:
\begin{itemize}
\item A heatmap, showing the frequency at which a method (row) outperforms on average another method (col). Methods are ordered by average such frequency, over all other methods.
The columns show the methods with the number of settings they were able to tackle (for example, some methods have no parallel version and therefore do not fill all settings).
\item A convergence curve, with the budget on the x-axis and the average (over all budgets) normalized (linearly, to 0-1) loss. Note that some benchmarks do not have the same functions for the different values of the budget. Therefore we might have a rugged curve, not monotonous at all. 
\end{itemize}
Note that the ranking of the rows or columns in the heatmap, and the ranking of the curves, do not have to match. As detailed above, runs for different budgets are independent, and we take all budget values in the statistics used for the heatmaps: therefore the ranking in the heatmap takes into account the low budget as much as the high budgets of the experiments.

Whereas most platforms do runs for a single budget, and then plot curves up to that budget, we do run the algorithms separately e.g. for budget 100, 200, 400 and 800. This implies that curves are less smooth. The reason for this is that smooth curves obtained by truncation can give a false sense of smoothness and falsify tests if users assume independance between results obtained for different values.

For noisy optimization, we assume unbiased noise in the artificial benchmarks. However, the real world ones are biased in the same sense as tuning in hyperparameter tuning: overfitting and underfitting can happen.
We differentiate ask, tell and recommend: this is critical. Some platforms do a simple ask and tell only and assume that
algorithms can, for free, guess which of their visited points is best. This is incorrect and misleading, as pointed out
in the noisy BBOB benchmark long ago~\cite{bbobissue1,bbobissue2,bbobissue3,bbobissue4}: instead of being based on a recommendation, the reported result is based on the minimum { {\em{expected}} fitness} over all visited points, as if it was possible to know (at cost zero) which of the visited points is the best.
More formally, noisy optimization algorithms typically have iterations defined by $(x_{n+1},\hat
x_{n+1})=Algorithm((x_1,\dots,x_n),(y_1,\dots,y_n))$. The $x_n$ are the iterates at which the noisy objective function
is evaluated, the $y_n$ are the noisy loss values, and the $\hat x_n$ are the recommendations, i.e. approximations of
the optimum as provided by the algorithm. In ask/tell format, $x_n$ is provided by ``ask'', the algorithm is informed of
$(x_n,y_n)$ by ``tell'' - and we need a method ``recommend'' for providing the recommendations. Ask and recommend are
distinct because $x_n$ and $\hat x_n$ are distinct. The regret is evaluated at $\hat x_n$, and it is
known~\cite{fabian,decocknoise} that, with a significant noise level, fast rates~\cite{fabian,chen1988} for the simple regret can only be obtained using $x_n$ far from the optimum (i.e. $x_n\neq \hat x_n$) for acquiring knowledge. Plotting results using $\E\inf_{i\leq n}(\E f)(x_i)$ (or any other criterion based on the $x_i$ rather than the $\hat x_n$) instead of $\E f(\hat x_n)$ is convenient for reusing noise-free software, but wrong: the best algorithms for such criteria are those which randomly explore around $\hat x_n$ rather than those which do clever explorations further from the optimum. We underline that, in spite of this bug for the noisy case, BBOB has been extremely useful for making benchmarking more rigorous in BBO.

``(RW)'' means that the benchmark is real world.

As pointed out during the seminar, ordered discrete is different from unordered discrete. Some of benchmarks include ordered discrete and some include unordered discrete: Nevergrad can use typed variables (as documented in \url{https://facebookresearch.github.io/nevergrad/optimization.html}) and we use this.

\section{Experiments}



