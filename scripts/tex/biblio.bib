@misc{bbobissue1,
author={Remi Coulom},
howpublished={\url{http://lists.lri.fr/pipermail/bbob-discuss/2012-April/000257.html}},
year={2012}} 
@misc{bbobissue2,
author={Hans-Georg Beyer},
howpublished={\url{http://lists.lri.fr/pipermail/bbob-discuss/2012-April/000270.html}},
year={2012}}
@misc{bbobissue3,
author={Hans-Georg Beyer},
howpublished={\url{http://lists.lri.fr/pipermail/bbob-discuss/2012-April/000258.html}},
year={2012}
}
@misc{bbobissue4,
author={Remi Coulom},
howpublished={\url{http://lists.lri.fr/pipermail/bbob-discuss/2012-April/000252.html}},
year={2012}
}

@inproceedings{decocknoise,
 author = {Decock, J{\'e}r{\'e}mie and Teytaud, Olivier},
 title = {Noisy Optimization Complexity Under Locality Assumption},
 booktitle = {Proceedings of the Twelfth Workshop on Foundations of Genetic Algorithms XII},
 series = {FOGA XII '13},
 year = {2013},
 unusedisbn =  {978-1-4503-1990-4},
 unusedunusedlocation = {Adelaide, Australia},
 pages = {183--190},
 numpages = {8},
 unusedurl = {http://doi.acm.org/10.1145/2460239.2460256},
 unusedunuseddoi =  {10.1145/2460239.2460256},
 acmid = {2460256},
 publisher = {ACM},
 unusedunusedaddress = {New York, NY, USA},
 unusedkeywords = {black box complexity model, local sampling, noisy optimization},
} 
@article{fabian,
author = "Fabian, Vaclav",
unusedunuseddoi =  "10.1214/aoms/1177699070", 
fjournal = "The Annals of Mathematical Statistics",
journal = "Ann. Math. Statist.",
month = "02",
number = "1",  
pages = "191--200",
publisher = "The Institute of Mathematical Statistics",
title = "Stochastic Approximation of Minima with Improved Asymptotic Speed",
unusedurl = "https://doi.org/10.1214/aoms/1177699070",
volume = "38",
year = "1967"
}

   

@article{chen1988, 
        author = "Chen, Hung",
        unusedunuseddoi =  "10.1214/aos/1176350965",
        journal = "The Annals of Statistics",
        month = "Sep",
        number = "3",
        pages = "1330--1334", 
        publisher = "The Institute of Mathematical Statistics",
        title = "Lower Rate of Convergence for Locating a Maximum of a Function",
        unusedurl = "http://dx.doi.org/10.1214/aos/1176350965",
        volume = "16",
        year = "1988"
}


@misc{micropredictions2,
   author = {MicroPredictions}, 
   author = {Petter Cotton},
   title = {MicroPredictions ELO ratings},
   year = "2020",    
   howpublished =  {\url{https://microprediction.github.io/optimizer-elo-ratings/}},
   unusednote = "[Online; accessed 27-April-2021]",
}       
        
@misc{micropredictions1,
   author = {Petter Cotton},
   title = {An introduction to Z-streams (and collective micropredictions)},
   year = "2020",
   howpublished =  {\url{https://www.linkedin.com/pulse/short-introduction-z-streams-peter-cotton-phd/}},
   unusednote = "[Online; accessed 27-March-2021]"
 }      

@book{rechenberg73,
        title        = {Evolutionstrategie: Optimierung Technischer Systeme nach Prinzipien des Biologischen Evolution},
        author       = {Ingo Rechenberg},
        year         = 1973,
        publisher    = {Fromman-Holzboog Verlag},
        unusedaddress = {Stuttgart}
}


@inproceedings{lengler,
author = {Doerr, Benjamin and Doerr, Carola and Lengler, Johannes},
title = {Self-Adjusting Mutation Rates with Provably Optimal Success Rules},
year = {2019},
unusedisbn = {9781450361118},
publisher = {Association for Computing Machinery},
unusedaddress = {New York, NY, USA},
unusedurl = {https://doi.org/10.1145/3321707.3321733},
unusedunuseddoi = {10.1145/3321707.3321733},
abstract = {The one-fifth success rule is one of the best-known and most widely accepted techniques to control the parameters of evolutionary algorithms. While it is often applied in the literal sense, a common interpretation sees the one-fifth success rule as a family of success-based updated rules that are determined by an update strength F and a success rate s. We analyze in this work how the performance of the (1+1) Evolutionary Algorithm (EA) on LeadingOnes depends on these two hyper-parameters. Our main result shows that the best performance is obtained for small update strengths F = 1+o(1) and success rate 1/e. We also prove that the runtime obtained by this parameter setting is asymptotically optimal among all dynamic choices of the mutation rate for the (1+1) EA, up to lower order error terms. We show similar results for the resampling variant of the (1+1) EA, which enforces to flip at least one bit per iteration.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1479â€“1487}, 
numpages = {9},
unusedlocation = {Prague, Czech Republic},
series = {GECCO '19} 
}       
